
machine(L1Cache, "MSI L1 Cache")
: Sequencer * sequencer,
  CacheMemory * cacheMemory, //ECE552: Unified L1 Instruction and Data Cache
  int cache_response_latency = 12,
  int issue_latency = 2,
  bool send_evictions = true
{

  // NETWORK BUFFERS

  //=============================================================
  // To the Network
  // ==============
  //ECE552: <requestFromCache>: The local cache coherence controller gets requests from the L1 cache (e.g., on a miss) that it will propagate to the network via virtual network 2.
  //ECE552: <responseFromCache>: The local cache coherence controller also propagates responses from its local cache to the network (e.g., a response to a forwarded GetS request from the directory)

  MessageBuffer requestFromCache, network="To", virtual_network="2", ordered="true", vnet_type="request";
  MessageBuffer responseFromCache, network="To", virtual_network="4", ordered="true", vnet_type="response";

  //=============================================================
  // From the Network
  // ================
  //ECE552:  <forwardToCache>: The local cache coherence controller receives forwarded requests from the directory.
  //ECE552:  <responseToCache>: The local cache coherence controller also receives replies from the Network for requests it issues via virtual network 2.

  MessageBuffer forwardToCache, network="From", virtual_network="3", ordered="true", vnet_type="forward";
  MessageBuffer responseToCache, network="From", virtual_network="4", ordered="true", vnet_type="response";

  //=============================================================
  // STATES
  // ======
  //
  // ECE552: AccessPermissions specified in RubySlicc_Exports.sm
  // ECE552: Valid Data:   Read_Only, Read_Write
  // ECE552: Invalid Data: Invalid, NotPresent, Busy
  //=============================================================

  state_declaration(State, desc="Cache states", default="L1Cache_State_I") { //ECE552: All cache blocks are initially in Invalid State

    //=====================
    //ECE552: Stable States
    //=====================

    I, AccessPermission:Invalid, desc="Not Present/Invalid";

    /* ECE552: Complete this section here */
    S, AccessPermission:Read_Only, desc="Shared stable state, block shared and read only";
    M, AccessPermission:Read_Write, desc="Modified stable state, owned and read writable";
     
    //========================
    //ECE552: Transient States
    //========================
    
    IS_D, AccessPermission:Busy, desc="Invalid to Shared waiting on Data from owner(dir or remote)";
    IM_AD, AccessPermission:Busy, desc="Invalid to Modified waiting on Data from owner(dir or remote)  and Inv_Acks(from sharers remaining)";
    IM_A, AccessPermission:Busy, desc="Invalid to Modified, recieved data waiting for Inv_Acks(from sharers remaining)";
    SM_AD, AccessPermission:Busy, desc="Shared to Modified,  waiting on Data from owner(dir or remote) and Inv_Acks(from sharers remaining)";
    SM_A, AccessPermission:Busy, desc="Shared to Modified, recieved data waiting for Inv_Acks(from sharers remaining)";
    MI_A, AccessPermission:Busy, desc="Modified to Invalid, waiting for Put-Ack from directory, remains owner of block";
    SI_A, AccessPermission:Busy, desc="Shared to Invalid, waiting for Put-Ack from directory, remains sharer, has to respond to Invalidations with Inv_Acks";
    II_A, AccessPermission:Busy, desc="Invalid, waiting for Put-Ack from directory";    
    /* ECE552: Complete this section here */

  }

  // =============================================================================
  // EVENTS
  // =============================================================================
  enumeration(Event, desc="Cache events") {

    //=================================================================================================================
    //ECE552: L1 Events triggered from the local processor explicitly (i.e., memory request) or implicitly (replacement)
    //=================================================================================================================
    Load,       desc="Load request from processor";
    Ifetch,     desc="Ifetch request from processor";
    Store,      desc="Store request from processor";

    Replacement,  desc="Replace L1 cache block";

    //========================================================================
    //ECE552: Requests forwarded to the L1 cache from the directory controller 
    //========================================================================
    Fwd_GetS,   desc="GetS forwarded from the directory"; //ECE552: original request was for read access
    Fwd_GetM,   desc="GetM forwarded from the directory"; //ECE552: original request was for write access
    Inv,        desc="Invalidate request from the directory";

    WB_Ack,     desc="Ack from the directory for a writeback";
    
    Inv_Ack,	 desc="Invalidate Ack";
    Inv_Ack_all, desc="Last Invalidate Ack";

    Data_from_Dir_No_Acks,       desc="Data from Directory, 0 Acks";
    Data_from_Dir_Ack_Cnt,       desc="Data from Directory, along with Ack Count";
    Data_from_Dir_Ack_Cnt_Last,  desc="Data from Directory, along with Ack Count, arrive after Inv_Ack";
    Data_from_Owner,		 desc="Data from remote L1 (remote Owner)";

  }

  // =============================================================================
  // STRUCTURE DEFINITIONS
  // =============================================================================

  MessageBuffer mandatoryQueue, ordered="false";

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry") {
    State CacheState,        desc="cache state";
    bool Dirty,              desc="Is the data dirty (different than memory)?";
    DataBlock DataBlk,       desc="Data in the block";
  }

  // TBE fields
  // ECE552: Transient Block Entry acts like an MSHR (i.e., Miss Handling Register) keeping track of transitions that require an intermediate transient state
  structure(TBE, desc="...") {
    State TBEState,               desc="Transient state";
    DataBlock DataBlk,       	  desc="data for the block, required for concurrent writebacks";
    int pendingAcks, default="0", desc="number of pending acks";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }


  // =============================================================================
  // STRUCTURES
  // =============================================================================
  TBETable TBEs, template_hack="<L1Cache_TBE>";

  // =============================================================================
  // PROTOTYPES
  // =============================================================================
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE b);
  void unset_tbe();

  Entry getCacheEntry(Address address), return_by_pointer="yes" {
    return static_cast(Entry, "pointer", cacheMemory.lookup(address));
  }

  // =============================================================================
  // FUNCTIONS
  // =============================================================================

  //ECE552: This function maps a Ruby Incoming Memory Request Type to a SLICC Event
  Event mandatory_request_type_to_event(RubyRequestType type) {
   if (type == RubyRequestType:LD) {
      return Event:Load;
    } else if (type == RubyRequestType:IFETCH) {
      return Event:Ifetch;
    } else if ((type == RubyRequestType:ST) || (type == RubyRequestType:ATOMIC)) {
      return Event:Store;
    } else {
      error("Invalid RubyRequestType");
    }
  }

  //ECE552: This function returns the state for memory location <addr>. Note the priority:
  // We first check the TBE (in case there are pending requests), then the cache, 
  // and if nothing is present then we return an invalid state.

  State getState(TBE tbe, Entry cache_entry, Address addr) {

    if (is_valid(tbe)) {
      return tbe.TBEState;
    }
    else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    else {
      return State:I;
    }
  }

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {

    if (is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  // ECE552: Function L1Cache_State_to_permission() is autogenerated here: build/ALPHA/mem/protocol/L1Cache_State.cc
  // ECE552: The permissions are the ones set in the States part earlier in this file.
  // ECE552: Again note the order the different structures are checked: first the tbe and then the cache

  AccessPermission getAccessPermission(Address addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      return L1Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      return L1Cache_State_to_permission(cache_entry.CacheState);
    }

    return AccessPermission:NotPresent;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L1Cache_State_to_permission(state));
    }
  }

  DataBlock getDataBlock(Address addr), return_by_ref="yes" {
    return getCacheEntry(addr).DataBlk;
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  GenericMachineType getNondirectHitMachType(MachineID sender) {
    if (machineIDToMachineType(sender) == MachineType:L1Cache) {
      //
      // NOTE direct local hits should not call this
      //
      return GenericMachineType:L1Cache_wCC; 
    } else {
      return ConvertMachToGenericMach(machineIDToMachineType(sender));
    }
  }


  // =============================================================================
  // NETWORK PORTS
  // =============================================================================


  //ECE552: Two output port that place messages onto the network (i.e., requests and reponses)
  out_port(requestNetwork_out, RequestMsg, requestFromCache);
  out_port(responseNetwork_out, ResponseMsg, responseFromCache);
 
  //ECE552: Process incoming request messages from the forwardToCache Network buffer. Trigger appropriate events based on the incoming CoherenceRequestType.
  //ECE552: All CoherenceRequestType declarations are provided in MSI-msg.sm

  in_port(forwardRequestNetwork_in, RequestMsg, forwardToCache) {
    if (forwardRequestNetwork_in.isReady()) {
      peek(forwardRequestNetwork_in, RequestMsg, block_on="Address") {

        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := TBEs[in_msg.Address];

        if (in_msg.Type == CoherenceRequestType:GETM) {
          DPRINTF(RubySlicc, "LOOK AT ME L1 - forwardRequestNetwork_in port: GETM request for address %s\n", in_msg.Address);
          trigger(Event:Fwd_GetM, in_msg.Address, cache_entry, tbe);
        }
	else if (in_msg.Type == CoherenceRequestType:GETS) {
          DPRINTF(RubySlicc, "L1 - forwardRequestNetwork_in port: GETS request for address %s\n", in_msg.Address);
          trigger(Event:Fwd_GetS, in_msg.Address, cache_entry, tbe);
        }
        else if (in_msg.Type == CoherenceRequestType:INV) {
          DPRINTF(RubySlicc, "L1 - forwardRequestNetwork_in port: INV request for address %s\n", in_msg.Address);
          trigger(Event:Inv, in_msg.Address, cache_entry, tbe);
        }
        else if (in_msg.Type == CoherenceRequestType:WB_ACK) {
          DPRINTF(RubySlicc, "L1 - forwardRequestNetwork_in port: WB_ACK messsage for address %s\n", in_msg.Address);
          trigger(Event:WB_Ack, in_msg.Address, cache_entry, tbe);
        }
        else {
          DPRINTF(RubySlicc, "Error! L1 - Unexpected Forwarded Message => address: %s, Request Type: %s\n", in_msg.Address, in_msg.Type);
          error("MSI_example L1 - Unexpected message in forwardToCache buffer");
        }
      }
    }
  }

  //ECE552: Process incoming response messages from the responseToCache Network buffer. Trigger appropriate events based on the incoming CoherenceResponseType.
  //ECE552: All CoherenceResponseType declarations are provided in MSI-msg.sm

  in_port(responseNetwork_in, ResponseMsg, responseToCache) {
    if (responseNetwork_in.isReady()) {
      peek(responseNetwork_in, ResponseMsg, block_on="Address") {

        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := TBEs[in_msg.Address];

        if (in_msg.Type == CoherenceResponseType:DATA_FROM_DIR) {
	  if (in_msg.AckCount == 0) {
            DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_DIR_NO_ACKS reply for address %s data %s\n", in_msg.Address, in_msg.DataBlk);
            trigger(Event:Data_from_Dir_No_Acks, in_msg.Address, cache_entry, tbe);
	  } else {
            if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
               DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_DIR_ACK_CNT reply for address %s - Data arrived after last Inv_Ack\n", in_msg.Address);
               trigger(Event:Data_from_Dir_Ack_Cnt_Last, in_msg.Address, cache_entry, tbe);
            } else {
               DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_DIR_ACK_CNT reply for address %s\n", in_msg.Address);
               trigger(Event:Data_from_Dir_Ack_Cnt, in_msg.Address, cache_entry, tbe);
            }
	  }
        }
        else if (in_msg.Type == CoherenceResponseType:DATA_FROM_OWNER) {
          DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_OWNER reply for address %s\n", in_msg.Address);
          trigger(Event:Data_from_Owner, in_msg.Address, cache_entry, tbe);
        } 
	else if (in_msg.Type == CoherenceResponseType:ACK) {
          if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            DPRINTF(RubySlicc, "L1 - responseNetwork_in port: INV_Ack_all for address %s\n", in_msg.Address);
            trigger(Event:Inv_Ack_all, in_msg.Address, cache_entry, tbe);
          } else {
            DPRINTF(RubySlicc, "L1 - responseNetwork_in port: INV_Ack for address %s\n", in_msg.Address);
            trigger(Event:Inv_Ack, in_msg.Address, cache_entry, tbe);
          }
	}
        else {
          DPRINTF(RubySlicc, "Error! L1 - Unexpected Coherence Response Message => address: %s, Request Type: %s\n", in_msg.Address, in_msg.Type);
          error("MSI_example L1 - Unexpected message in reponseToCache buffer");
        }
      }
    }
  }

  //Mandatory Queue
  //ECE552: Process requests from the local processor.

  in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...") {
    if (mandatoryQueue_in.isReady()) {
      peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {

        Entry cache_entry := getCacheEntry(in_msg.LineAddress);
        if (is_invalid(cache_entry) &&
            cacheMemory.cacheAvail(in_msg.LineAddress) == false ) {
          DPRINTF(RubySlicc, "L1 - mandatoryQueue_in request for address %s - Need to replace entry\n", in_msg.LineAddress);
          // make room for the block
          trigger(Event:Replacement, cacheMemory.cacheProbe(in_msg.LineAddress),
                  getCacheEntry(cacheMemory.cacheProbe(in_msg.LineAddress)),
                  TBEs[cacheMemory.cacheProbe(in_msg.LineAddress)]);
        }
        else {
          DPRINTF(RubySlicc, "L1 - mandatoryQueue_in request of type %s for address %s - No replacement necessary\n", in_msg.Type, in_msg.LineAddress);
          trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                  cache_entry, TBEs[in_msg.LineAddress]);
        }
      }
    }
  }

  // =============================================================================
  // ACTIONS
  // =============================================================================

  /* ECE552: A request is issued on a cache miss. Note that we should issue a different msg type
     based on original request (i.e., GETM for writes and GETS for reads).
     We need to specify the requestor core so we know where the data/ack counts etc will return to.
     We also need to specify the destination, assuming we have more than one directory controllers.
  */

  action(as_issueGetS, "as", desc="Issue a request for read-only access") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:GETS;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.MessageSize := MessageSizeType:Control;
    }
  }

  action(am_issueGetM, "am", desc="Issue a request for read-write access") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:GETM;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.MessageSize := MessageSizeType:Control;
    }
  }

  /* ECE552: Issue PUTS/PUTM requests, on local replacements to the directory */

  action(bs_issuePUTS, "bs", desc="Issue a PUTS request") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTS;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.MessageSize := MessageSizeType:Data;
    }
  }

  
  action(bm_issuePUTM, "bm", desc="Issue a PUTM request") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTM;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.MessageSize := MessageSizeType:Data;
    }
  }

  action(e_sendDataFromCacheToRequestor, "e", desc="Send data from cache to requestor") {
    peek(forwardRequestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_FROM_OWNER;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(ee_sendDataFromTBEToRequestor, "\e", desc="Send data from TBE to requestor") {
    peek(forwardRequestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        assert(is_valid(tbe));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_FROM_OWNER;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(de_sendDataFromCacheToDir, "de", desc="Send data from cache to directory") { 
     enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) { 
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(map_Address_to_Directory(address));
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
     }
  }

  action(dee_sendDataFromTBEToDir, "d\e", desc="Send data from TBE to directory") {
     enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        assert(is_valid(tbe));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(map_Address_to_Directory(address));
        out_msg.DataBlk := tbe.DataBlk;
	out_msg.MessageSize := MessageSizeType:Response_Data;
     }
  }

  action(fi_sendInvAck, "fi", desc="send Invalidation Acknowledgement to Requestor") {
    peek(forwardRequestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }
  }


  action(i_allocateL1CacheBlock, "i", desc="Allocate a cache block") {
    if (is_valid(cache_entry)) {
      if (getAccessPermission(address) != AccessPermission:Invalid) {
	error("Should not get an allocate request for a block that has not Invalid permissions");
      }
    } else {
      set_cache_entry(cacheMemory.allocate(address, new Entry));
    }
  }

  action(h_deallocateL1CacheBlock, "h", desc="deallocate a cache block") {
    if (is_valid(cache_entry)) {
      cacheMemory.deallocate(address);
      unset_cache_entry();
    }
  }

  //ECE552: remove request issued by the local core from the mandatory incoming queue
  action(m_popMandatoryQueue, "m", desc="Pop the mandatory request queue") {
    mandatoryQueue_in.dequeue();
  }

  action(n_popResponseQueue, "n", desc="Pop the response queue") {
    
    profileMsgDelay(1, responseNetwork_in.dequeue_getDelayCycles());

  }

  action(o_popForwardedRequestQueue, "o", desc="Pop the forwarded request queue") {
    profileMsgDelay(2, forwardRequestNetwork_in.dequeue_getDelayCycles());
  }

  action(p_profileMiss, "p", desc="Profile cache miss") {
    peek(mandatoryQueue_in, RubyRequest) {
      cacheMemory.profileMiss(in_msg);
    }
  }

  /* ECE552: Load completed. Data block was present with Read permissions in local cache */
  action(r_load_hit, "r", desc="Notify sequencer the load completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
    sequencer.readCallback(address, 
                           GenericMachineType:L1Cache,
                           cache_entry.DataBlk);
  }

  /* ECE552: Load completed. Block was not present in the local cache. 
     Got data either from the the directory or from another remote L1 cache */
  action(rx_load_hit, "rx", desc="External load completed.") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
      sequencer.readCallback(address, 
                             getNondirectHitMachType(in_msg.Sender),
                             cache_entry.DataBlk);
    }
  }

  /* ECE552: Store completed. Data block was present with Write permissions in local cache */
  action(s_store_hit, "s", desc="Notify sequencer that store completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
    sequencer.writeCallback(address, 
                            GenericMachineType:L1Cache,
                            cache_entry.DataBlk);
  }

  /* ECE552: Store completed. Block was not present in the local cache or had Read_Only permissions. 
     Got data either from the the directory or from the previous owner of that block (i.e., a remote L1 cache) */
  action(sx_store_hit, "sx", desc="External store completed.") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
      sequencer.writeCallback(address, 
                              getNondirectHitMachType(in_msg.Sender),
                              cache_entry.DataBlk);
    }
  }

  /* ECE552: Copy data block to the cache entry. Note that the is_valid(cache_entry)
     does not check if the block is valid, but rather if it has been allocated (i.e., !NULL pointer)
  */
  action(u_writeDataToCache, "u", desc="Write data to the cache") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
    }
  }

  /* ECE552: Update the number of pending acknowledgements in the TBE entry.
     When a TBE entry gets allocates pendingAcks is set to 0. The in_msg.AckCount
     when received from the directory is a negative number (i.e., number of sharers)
  */
  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  /* ECE552: let the sequencer know that the cache replaced a block. */
  action(forward_eviction_to_cpu, "\cc", desc="sends eviction information to the processor") {
    DPRINTF(RubySlicc, "forward_eviction to cpu; send_evictions is %s\n", send_evictions);
    if (send_evictions) {
      DPRINTF(RubySlicc, "Sending invalidation for %s to the CPU\n", address);
      sequencer.evictionCallback(address);
    }
  }

  /* ECE552: allocate a TBE entry. The block is in a transient coherence state. */
  action(v_allocateTBE, "v", desc="Allocate TBE") {
    TBEs.allocate(address);
    set_tbe(TBEs[address]); //Where is the body of set_tbe()?
 			    // It's here (a) mem/slicc/ast/FuncCallExprAST.py
                            //       and (b) mem/slicc/symbols/StateMachine.py
  }

  /* ECE552: Deallocate the TBE entry. The block is now in a stable coherence state and resides in the cache. */
  action(w_deallocateTBE, "w", desc="Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  /* ECE552: Copy Data block from cache entry to TBE. Remember a block in transient state
     might still be asked to provide data to forwarded requests */
  action(x_copyDataFromCacheToTBE, "x", desc="Copy data from cache to TBE") {
    assert(is_valid(cache_entry));
    assert(is_valid(tbe));
    tbe.DataBlk := cache_entry.DataBlk;
  }

  /* ECE552: Do nothing. Just stall. */
  action(z_stall, "z", desc="stall") {
    // do nothing
  }

  // =============================================================================
  // TRANSITIONS
  // =============================================================================

  /* ECE552: Complete this section here */
 
  //enumeration(Event, desc="Cache events") {

  //  //=================================================================================================================
  //  //ECE552: L1 Events triggered from the local processor explicitly (i.e., memory request) or implicitly (replacement)
  //  //=================================================================================================================
  //  Load       desc="Load request from processor";
  //  Ifetch,     desc="Ifetch request from processor";
  //  Store,      desc="Store request from processor";

  //  Replacement,  desc="Replace L1 cache block";

  //  //========================================================================
  //  //ECE552: Requests forwarded to the L1 cache from the directory controller 
  //  //========================================================================
  //  Fwd_GetS,   desc="GetS forwarded from the directory"; //ECE552: original request was for read access
  //  Fwd_GetM,   desc="GetM forwarded from the directory"; //ECE552: original request was for write access
  //  Inv,        desc="Invalidate request from the directory";

  //  WB_Ack,     desc="Ack from the directory for a writeback";
  //  
  //  Inv_Ack,	 desc="Invalidate Ack";
  //  Inv_Ack_all, desc="Last Invalidate Ack";

  //  Data_from_Dir_No_Acks,       desc="Data from Directory, 0 Acks";
  //  Data_from_Dir_Ack_Cnt,       desc="Data from Directory, along with Ack Count";
  //  Data_from_Dir_Ack_Cnt_Last,  desc="Data from Directory, along with Ack Count, arrive after Inv_Ack";
  //  Data_from_Owner,		 desc="Data from remote L1 (remote Owner)";

  //}



  //Cache wants to issue a load or an IFETCH on an invalid cache line
  //send a GetS signal to Dir
  //Go to a transient state and then
  //Wait for Dir to send back the data  
  transition(I, {Load, Ifetch}, IS_D){
    //send GetS to Dir
    v_allocateTBE;
    i_allocateL1CacheBlock;
    p_profileMiss;
    as_issueGetS;
    m_popMandatoryQueue;
  }
  
  //Cache wants to issue a store on an invalid cache line
  //send a GetM to dir
  //Wait for dir to send the rest of the cache line before we can do the store
  transition(I, Store, IM_AD){
    //send GetM to Dir
    v_allocateTBE;
    i_allocateL1CacheBlock;
    p_profileMiss;
    am_issueGetM;
    m_popMandatoryQueue;
  }
  


  //transient state between Invalid and shared state
  //waiting for data, on anything else we are stalling
  transition(IS_D, {Load, Ifetch, Store, Replacement, Inv},IS_D){
    //stall  
    z_stall;
   }  
  

  //transient state between Invalid and shared state
  //received the data from the directory (on the response queue)
  //now ready to go to shared state after writing the data to cache and recording the hit
  transition(IS_D, {Data_from_Dir_No_Acks, Data_from_Owner}, S){
    //go to shared
    u_writeDataToCache;
    rx_load_hit;
    w_deallocateTBE;
    n_popResponseQueue;
   }  
  
  //transient state between Invalid and shared state
  //waiting for data, on anything else we are stalling
  transition(IM_AD, {Load,Ifetch, Store, Replacement, Fwd_GetS, Fwd_GetM}, IM_AD){
    //stall  
    z_stall;
   }  
  
  //transient state between Invalid and modified state
  //received the data from the directory (on the response queue), with no more acknowledgements
  //we can transition to modified directly as no other cache's have this block, after writing data to cache and recording the hit
  transition(IM_AD, {Data_from_Dir_No_Acks, Data_from_Owner}, M){
     //go to modified 
    u_writeDataToCache;
    sx_store_hit;
    w_deallocateTBE;
    n_popResponseQueue;
   }
  
  //transient state between Invalid and modified state
  //received the data from the directory (on the response queue), but there are still pending acknowlegements
  //write the data to cache and update the acknowlegement count
  //waiting for the rest of the acknowledgments (when all other caches have invalidated) before it is safe to transition to Modified
  transition(IM_AD,Data_from_Dir_Ack_Cnt, IM_A){
    //go to IM_A  
    u_writeDataToCache;
    q_updateAckCount;
    n_popResponseQueue;
      
  } 
 
 
  //transient state between invalid and modified
  //received the data from the directory with the last of the acknowledgments
  //write the data to cache, record the hit and transition to modified since no more acknowledgements left
  transition(IM_AD, Data_from_Dir_Ack_Cnt_Last, M){
    //go to M
    u_writeDataToCache;
    sx_store_hit;
    w_deallocateTBE;
    n_popResponseQueue;


  }

  //transient state between invalid and modified
  //waiting for data from directory (after getm)
  //received an invalidation from one of the caches (after invalidating), update the ackcount, and continue waiting
  //for the data and rest of the acknowledgments  
  transition(IM_AD, Inv_Ack, IM_AD){
     //acks --
     q_updateAckCount;
     n_popResponseQueue;

  }


  //transient state between invalid and modified
  //waiting for the remaining acknowledgments, already received the data
  //anything else stall
  transition(IM_A, {Load,Ifetch, Store, Replacement, Fwd_GetS, Fwd_GetM}, IM_A){
     //stall
    z_stall;
  }


  //transient state between invalid and modified
  //already received the data
  //waiting for the rest of the acknowledgements
  //received one of the acknowledgements and update the ack count
  //continue to wait for the rest of the acknowledgements
  transition(IM_A, Inv_Ack, IM_A){
     //acks --
     q_updateAckCount;
     n_popResponseQueue;
  }


  //transient state between invalid and modified
  //already received the data
  //receieved the last of the pending acknowledgements
  //now can record the store hit and transition to modified
  transition(IM_A, Inv_Ack_all, M){
     //go to modified
    sx_store_hit;
    w_deallocateTBE;
    n_popResponseQueue;

  }

  //cache is already in shared, and a load is issued, hit!
  transition(S, {Load,Ifetch} , S){
     //hit
     r_load_hit;
     m_popMandatoryQueue;
  }
 
 
  //cache is in shared, and now cache wants to do a store (upgrade miss)
  //send a getm to dir to request ownership of this block
  //now have to wait for data from directory and acknowledgment that all the other caches invalidated (need exclusive copy) 
  transition(S, Store, SM_AD){
     //send GETM to dir
    v_allocateTBE;
    am_issueGetM;
    p_profileMiss;
    m_popMandatoryQueue;
  }
 
 
 
  //cache is in shared and now about to be evicted
  //issue a PUTS to directory (to remove from sharers list), and wait for acknowledgment for directory on writeback (after dir writes to memory) 
  transition(S, Replacement, SI_A){
     //send PUTS to dir
    v_allocateTBE;
    bs_issuePUTS;

  }

    
  //cache in shared, received invalidation request from directory (on a getm from another cache that wants exclusivity)
  //can go straight to invalid as it is only a shared copy
  //send directory an invalidation acknowledgment
  transition(S, Inv, I){
     //send Inv-Ack to Requestor
     h_deallocateL1CacheBlock;
     forward_eviction_to_cpu;
     fi_sendInvAck;
     o_popForwardedRequestQueue;

  }

  
  //cache in transient state between shared and modified, recieved load and it can service load, hit!
  transition(SM_AD, {Load,Ifetch} , SM_AD){
     //hit
     r_load_hit;
     m_popMandatoryQueue;
  }

  //cache in transient state between shared and modified, waiting for acknowledgement and data from dir, stall elsewise
  transition(SM_AD, {Store, Replacement, Fwd_GetS, Fwd_GetM}, SM_AD){
     //stall
    z_stall;
  }


  //cache in transient state between shared and modified, recieved an invalidation request
  //send inv-ack and still wait for data and acks before going to modify yourself 
  transition(SM_AD, Inv, IM_AD){
     //send Inv-Ack to Requestor
     fi_sendInvAck;
     o_popForwardedRequestQueue;

  }


  //cache in transient state between shared and modified, waiting for data and acknowledgments
  //recieves data and now will only wait for the other caches to acknowledge their invalidations
  //write the data that is received to the cache
  transition(SM_AD, Data_from_Dir_Ack_Cnt ,  SM_A){
     //go to SM_A
     u_writeDataToCache;
     q_updateAckCount;
     n_popResponseQueue;
  }
 
 

  //cache in transient state between shared and modified, receivied the last data or data from owner
  //we know there are no more acknowledgements so we can transition to modified directly
  //copy the data to cache and record the hit
  transition(SM_AD, {Data_from_Dir_Ack_Cnt_Last, Data_from_Dir_No_Acks, Data_from_Owner},  M){
 
    u_writeDataToCache;
    sx_store_hit;
    w_deallocateTBE;
    n_popResponseQueue;
 
   } 


  //cache in transient state between shared and modified, waiting for data and acknowledgments
  //one of the acknowledgements came in so we decrement the counter
  transition(SM_AD, Inv_Ack, SM_AD){
     //acks --
     q_updateAckCount;
     n_popResponseQueue;

  }


  //cache in transient state between shared and modified, already received data and waiting for acknowledgments
  //still technically in shared state (not removed from sharers yet), can service loads and ifetches, hit!
  transition(SM_A, {Load,Ifetch} , SM_A){
     //hit
     r_load_hit;
     m_popMandatoryQueue;
  
  }


  //cache in transient state between shared and modified, received data, waiting for acknowledgments
  //no acknowledgments,or load requests, stall
  transition(SM_A, {Store, Replacement, Fwd_GetS, Fwd_GetM}, SM_A){
     //stall
    z_stall;
  }
  
  //cache in transient state between shared and modified, received data and just received one of the acknowledgments
  //decrement the acknowledgment counter
  transition(SM_A, Inv_Ack, SM_A){
     //acks --
     q_updateAckCount;
     n_popResponseQueue;
  }
  

  //cache in transient state between shared and modified,received the data and now just received the last invalidation acknowledgment
  //transition to modified
  transition(SM_A, Inv_Ack_all, M){
     //go to modified
    sx_store_hit;
    w_deallocateTBE;
    n_popResponseQueue;
  }
  

  //cache in modified, and load, ifetch issued
  //hit!
  transition(M, {Load,Ifetch} ,  M){
     //hit
     r_load_hit;
     m_popMandatoryQueue;
  }

  //cache in modified, and store issued
  //hit!
  transition(M, Store,  M){
     //hit
     s_store_hit;
     m_popMandatoryQueue;
  }


  //cache in modified, and replacement issued,
  //wait for acknowledgment from dir(memory writeback acknowledgmnt)
  transition(M, Replacement,  MI_A){
     //send PUTM and Data to dir
    v_allocateTBE;
    x_copyDataFromCacheToTBE;
    bm_issuePUTM;
  }

  //cache in modified, someone else wants a shared copy
  //send data over (to requestor and the directory) and transition to the shared state
  transition(M, Fwd_GetS,  S){
     //send Data to Requestor and Dir
     e_sendDataFromCacheToRequestor;
     de_sendDataFromCacheToDir;
     o_popForwardedRequestQueue; 

  }

  //cache in modified, someone else wants a modified copy
  //send data over to requestor (directory doesn't need it, as only one owner) and transition to invalid

  transition(M, Fwd_GetM,  I){
     //send Data to Requestor 
     e_sendDataFromCacheToRequestor;
     h_deallocateL1CacheBlock;
     forward_eviction_to_cpu;
     o_popForwardedRequestQueue; 
  }

  //transient state between modified and invalid, waiting for writeback acknowledgment after M state was issued a replacement
  //stalling if not acknowledgment or forward request(we still have the data until writeback is complete)
  transition(MI_A, {Load,Ifetch, Store, Replacement},  MI_A){
     //stall 
    z_stall;
  }

  //transient state between modified and invalid, waiting for writeback acknowledgment
  //someone else wants a shared copy, since we still have data we forward to data
  //transition to SI_A state (have shared copy and waiting for WB acknowledgment before invalid)
  //now no longer the owner
  transition(MI_A, Fwd_GetS,  SI_A){
     //send Data to Requestor and Dir
     ee_sendDataFromTBEToRequestor;
     dee_sendDataFromTBEToDir;
     o_popForwardedRequestQueue; 
  }


  //transient state between modified and invalid, waiting for writeback acknowledgment
  //someone else wants a modified copy, forward the data and relinquish ownership
  //still wait for acknowledgment before going to the invalid state
  transition(MI_A, Fwd_GetM,  II_A){
     //send Data to Requestor
     ee_sendDataFromTBEToRequestor;
     o_popForwardedRequestQueue; 
  }



  //transient state between shared and invalid, waiting for writeback acknowledgment
  //stall otherwise 
  transition(SI_A, {Load,Ifetch, Store, Replacement},  SI_A){
     //stall 
    z_stall;
  }

  //transient state between shared and invalid
  //received invalidation request from directory and issue an invalidation acknowledgment in return
  transition(SI_A, Inv, II_A){
     //send Inv-Ack to Requestor
     fi_sendInvAck;
     o_popForwardedRequestQueue;
  }

  //transient states waiting for the writeback acknowledgment from the directory before they can invalidate
  transition({MI_A,SI_A, II_A}, WB_Ack,  I){
     //go to invalidate
    h_deallocateL1CacheBlock;
    forward_eviction_to_cpu;
    w_deallocateTBE;
    o_popForwardedRequestQueue;
  }
  

  //transient state before invalidating waiting for writeback acknowledgments
  //stall otherwise
  transition(II_A, {Load,Ifetch, Store, Replacement},  II_A){
     //stall 
    z_stall;
  }

}
